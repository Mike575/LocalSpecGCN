{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import socket\n",
    "import importlib\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.getcwd()\n",
    "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
    "sys.path.append(BASE_DIR)\n",
    "#sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
    "import provider\n",
    "import part_dataset_all_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c07f39b953c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mNUM_POINT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFLAGS\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pointnet2_part_ssg_spec_2k_onehot'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'part_seg_spec_2k_onehot_w_normal_l164_l316'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# pointnet2_part_ssg_spec_2k_onehot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "\n",
    "#data_set = 'shapenet_part_seg_normals_curvature_hdf5_data'\n",
    "BATCH_SIZE = 32\n",
    "NUM_POINT = 2048\n",
    "\n",
    "model = 'pointnet2_part_ssg_spec_2k_onehot'\n",
    "log_dir = 'part_seg_spec_2k_onehot_w_normal_l164_l316'\n",
    "model_path = 'best_model_epoch_052.ckpt'\n",
    "# pointnet2_part_ssg_spec_2k_onehot\n",
    "# part_seg_spec_2k_onehot_w_normal_l164_l316\n",
    "\n",
    "MODEL_FILE = os.path.join(ROOT_DIR, 'models', model+'.py')\n",
    "LOG_DIR_prefix = '/usr/local/data/chuwang/cvpr18_rebuttal_exps/seg/2k_exp/'\n",
    "EXP_prefix = ''\n",
    "LOG_DIR = os.path.join(LOG_DIR_prefix,EXP_prefix,log_dir)\n",
    "sys.path.append(LOG_DIR)\n",
    "MODEL = importlib.import_module(model) # import network module\n",
    "MODEL_PATH = os.path.join(LOG_DIR_prefix,EXP_prefix,log_dir,model_path)\n",
    "\n",
    "NUM_CLASSES = 50\n",
    "\n",
    "# Shapenet official train/test split\n",
    "DATA_DIR = '/usr/local/data/chuwang/spec_pointnet_master/'\n",
    "DATA_PATH = os.path.join(DATA_DIR, 'data', 'shapenetcore_partanno_segmentation_benchmark_v0_normal')\n",
    "TRAIN_DATASET = part_dataset_all_normal.PartNormalDataset(root=DATA_PATH, npoints=NUM_POINT, classification=False, split='trainval', return_cls_label=True)\n",
    "TEST_DATASET = part_dataset_all_normal.PartNormalDataset(root=DATA_PATH, npoints=NUM_POINT, classification=False, split='test', return_cls_label=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save eigen info h5\n",
    "def save_h5_seg_normal(h5_filename, data, normal, seg_label, cls_label,\n",
    "    data_dtype='float32', label_dtype='uint8'):\n",
    "    h5_fout = h5py.File(h5_filename)\n",
    "    h5_fout.create_dataset(\n",
    "            'data', data=data,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'normal', data=normal,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'cls_label', data=cls_label,\n",
    "            compression='gzip', compression_opts=1,\n",
    "            dtype=label_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'seg_label', data=seg_label,\n",
    "            compression='gzip', compression_opts=1,\n",
    "            dtype=label_dtype)\n",
    "    \n",
    "    h5_fout.close()\n",
    "    \n",
    "def get_batch(dataset, idxs, start_idx, end_idx):\n",
    "    bsize = end_idx-start_idx\n",
    "    batch_data = np.zeros((bsize, NUM_POINT, 3))\n",
    "    batch_normal = np.zeros((bsize, NUM_POINT, 3))\n",
    "    batch_seg_label = np.zeros((bsize, NUM_POINT), dtype=np.int32)\n",
    "    batch_cls_label = np.zeros((bsize, 1), dtype=np.int32)\n",
    "    \n",
    "    for i in range(bsize):\n",
    "        point,normal,seg,cls = dataset[idxs[i+start_idx]]\n",
    "        batch_data[i,...] = point\n",
    "        batch_normal[i,...] = normal\n",
    "        batch_seg_label[i,:] = seg\n",
    "        batch_cls_label[i,:] = cls\n",
    "        \n",
    "        \n",
    "    return batch_data, batch_normal, batch_seg_label, batch_cls_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch:0\n",
      "Saving results -> ply_data_train0.h5...\n",
      "(2048, 2048, 3)\n",
      "processing batch:1\n",
      "Saving results -> ply_data_train1.h5...\n",
      "(2048, 2048, 3)\n",
      "processing batch:2\n",
      "Saving results -> ply_data_train2.h5...\n",
      "(2048, 2048, 3)\n",
      "processing batch:3\n",
      "Saving results -> ply_data_train3.h5...\n",
      "(2048, 2048, 3)\n",
      "processing batch:4\n",
      "Saving results -> ply_data_train4.h5...\n",
      "(2048, 2048, 3)\n",
      "processing batch:5\n",
      "Saving results -> ply_data_train5.h5...\n",
      "(1897, 2048, 3)\n"
     ]
    }
   ],
   "source": [
    "splitname = 'train'\n",
    "dataset = TRAIN_DATASET\n",
    "num_batches = len(dataset)/BATCH_SIZE\n",
    "idxs = np.arange(0, len(dataset))\n",
    "max_ind = idxs[-1]\n",
    "\n",
    "\n",
    "for batch_idx in range(num_batches+1):\n",
    "    print(\"processing batch:\" + str(batch_idx))\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "    \n",
    "    if end_idx > max_ind:\n",
    "        end_idx = max_ind + 1\n",
    "    \n",
    "    data, normal, seg, cls = get_batch(dataset, idxs, start_idx, end_idx)\n",
    "    \n",
    "    filename = 'ply_data_' + splitname + str(batch_idx) + '.h5'\n",
    "    \n",
    "    print(\"Saving results -> \" + filename + \"...\")\n",
    "    print(data.shape)\n",
    "    \n",
    "    h5_out = os.path.join(DATA_ROOT, 'data' ,\n",
    "                              'shapenet_part_seg_hdf5_data_normal' ,\n",
    "                              filename)\n",
    "    \n",
    "    save_h5_seg_normal(h5_out, \n",
    "                       data,\n",
    "                       normal, \n",
    "                       seg, \n",
    "                       cls\n",
    "                      )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch:0\n",
      "Saving results -> ply_data_val0.h5...\n",
      "(1870, 2048, 3)\n"
     ]
    }
   ],
   "source": [
    "splitname = 'val'\n",
    "dataset = VAL_DATASET\n",
    "num_batches = len(dataset)/BATCH_SIZE\n",
    "idxs = np.arange(0, len(dataset))\n",
    "max_ind = idxs[-1]\n",
    "\n",
    "\n",
    "for batch_idx in range(num_batches+1):\n",
    "    print(\"processing batch:\" + str(batch_idx))\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "    \n",
    "    if end_idx > max_ind:\n",
    "        end_idx = max_ind + 1\n",
    "    \n",
    "    data, normal, seg, cls = get_batch(dataset, idxs, start_idx, end_idx)\n",
    "    \n",
    "    filename = 'ply_data_' + splitname + str(batch_idx) + '.h5'\n",
    "    \n",
    "    print(\"Saving results -> \" + filename + \"...\")\n",
    "    print(data.shape)\n",
    "    \n",
    "    h5_out = os.path.join(DATA_ROOT, 'data' ,\n",
    "                              'shapenet_part_seg_hdf5_data_normal' ,\n",
    "                              filename)\n",
    "    \n",
    "    save_h5_seg_normal(h5_out, \n",
    "                       data,\n",
    "                       normal, \n",
    "                       seg, \n",
    "                       cls\n",
    "                      )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch:0\n",
      "Saving results -> ply_data_test0.h5...\n",
      "(2048, 2048, 3)\n",
      "processing batch:1\n",
      "Saving results -> ply_data_test1.h5...\n",
      "(826, 2048, 3)\n"
     ]
    }
   ],
   "source": [
    "splitname = 'test'\n",
    "dataset = TEST_DATASET\n",
    "num_batches = len(dataset)/BATCH_SIZE\n",
    "idxs = np.arange(0, len(dataset))\n",
    "max_ind = idxs[-1]\n",
    "\n",
    "\n",
    "for batch_idx in range(num_batches+1):\n",
    "    print(\"processing batch:\" + str(batch_idx))\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "    \n",
    "    if end_idx > max_ind:\n",
    "        end_idx = max_ind + 1\n",
    "    \n",
    "    data, normal, seg, cls = get_batch(dataset, idxs, start_idx, end_idx)\n",
    "    \n",
    "    filename = 'ply_data_' + splitname + str(batch_idx) + '.h5'\n",
    "    \n",
    "    print(\"Saving results -> \" + filename + \"...\")\n",
    "    print(data.shape)\n",
    "    \n",
    "    h5_out = os.path.join(DATA_ROOT, 'data' ,\n",
    "                              'shapenet_part_seg_hdf5_data_normal' ,\n",
    "                              filename)\n",
    "    \n",
    "    save_h5_seg_normal(h5_out, \n",
    "                       data,\n",
    "                       normal, \n",
    "                       seg, \n",
    "                       cls\n",
    "                      )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TRAIN_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2048 + 826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = os.path.join(DATA_ROOT, 'data' , data_set , TRAIN_FILES[0])\n",
    "filename = os.path.join(DATA_ROOT, 'data' , data_set , 'ply_data_val0.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write numpy array data and label to h5_filename\n",
    "def save_h5_data_label_normal(h5_filename, data, label, normal, \n",
    "    data_dtype='float32', label_dtype='uint8', noral_dtype='float32'):\n",
    "    h5_fout = h5py.File(h5_filename)\n",
    "    h5_fout.create_dataset(\n",
    "            'data', data=data,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'normal', data=normal,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=normal_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'label', data=label,\n",
    "            compression='gzip', compression_opts=1,\n",
    "            dtype=label_dtype)\n",
    "    h5_fout.close()\n",
    "\n",
    "\n",
    "# Write numpy array data and label to h5_filename\n",
    "def save_h5(h5_filename, data, label, data_dtype='uint8', label_dtype='uint8'):\n",
    "    h5_fout = h5py.File(h5_filename)\n",
    "    h5_fout.create_dataset(\n",
    "            'data', data=data,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'label', data=label,\n",
    "            compression='gzip', compression_opts=1,\n",
    "            dtype=label_dtype)\n",
    "    h5_fout.close()\n",
    "    \n",
    "\n",
    "# save eigen info h5\n",
    "def save_h5_data_eigen_meta(h5_filename, data, label, normal, egvect, egval,\n",
    "    data_dtype='float32', label_dtype='uint8'):\n",
    "    h5_fout = h5py.File(h5_filename)\n",
    "    h5_fout.create_dataset(\n",
    "            'data', data=data,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'normal', data=normal,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'label', data=label,\n",
    "            compression='gzip', compression_opts=1,\n",
    "            dtype=label_dtype)\n",
    "    \n",
    "    # add eigen meta info\n",
    "    h5_fout.create_dataset(\n",
    "            'egvect', data=egvect,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'egval', data=egval,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    \n",
    "    h5_fout.close()\n",
    "    \n",
    "\n",
    "# save eigen info h5\n",
    "def save_h5_data_curvature(h5_filename, data, label, normal, curvature,\n",
    "    data_dtype='float32', label_dtype='uint8'):\n",
    "    h5_fout = h5py.File(h5_filename)\n",
    "    h5_fout.create_dataset(\n",
    "            'data', data=data,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'normal', data=normal,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'label', data=label,\n",
    "            compression='gzip', compression_opts=1,\n",
    "            dtype=label_dtype)\n",
    "    \n",
    "    # add surface curvature info\n",
    "    # \\lamda0 / \\sum(\\lamda_i)\n",
    "    h5_fout.create_dataset(\n",
    "            'curvature', data=curvature,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    \n",
    "    h5_fout.close()\n",
    "\n",
    "def load_h5_eigen_meta(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    normal = f['normal'][:]\n",
    "    egvect = f['egvect'][:]\n",
    "    egval = f['egval'][:]\n",
    "    return (data, label, normal,egvect,egval)\n",
    "\n",
    "def load_h5_data_label_normal(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    normal = f['normal'][:]\n",
    "    return (data, label, normal)\n",
    "\n",
    "def load_h5_surf_curvature(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    normal = f['normal'][:]\n",
    "    curvature = f['curvature'][:]\n",
    "    return (data, label, normal,curvature)\n",
    "    \n",
    "def placeholder_inputs(batch_size, num_point):\n",
    "    pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3))\n",
    "    labels_pl = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    return pointclouds_pl, labels_pl\n",
    "\n",
    "\n",
    "def load_h5_data_label_seg(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    seg = f['pid'][:]\n",
    "    return (data, label, seg)\n",
    "\n",
    "def load_h5_data_label_seg_normal_curvature(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    seg = f['pid'][:]\n",
    "    normal = f['normal'][:]\n",
    "    curvature = f['curvature'][:]\n",
    "    return (data, label, seg,normal,curvature)\n",
    "\n",
    "def load_h5_data_seg_normal(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    normal = f['normal'][:]\n",
    "    cls_label = f['cls_label'][:]\n",
    "    seg_label = f['seg_label'][:]\n",
    "    \n",
    "    return (data, normal, seg_label, cls_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'shapenet_part_seg_hdf5_data_normal'\n",
    "filename = os.path.join(DATA_ROOT, 'data' , data_set , 'ply_data_test0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_h5_data_seg_normal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f44999f1ea16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_label\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mload_h5_data_seg_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_h5_data_seg_normal' is not defined"
     ]
    }
   ],
   "source": [
    "data, normal, seg_label, cls_label= load_h5_data_seg_normal(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'shapenet_part_seg_hdf5_data_normal'\n",
    "filename = os.path.join(DATA_ROOT, 'data' , data_set , 'ply_data_train3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, normal, seg_label, cls_label= load_h5_data_seg_normal(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_, label_, seg_= load_h5_data_label_seg(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label is just object class label\n",
    "print(label.shape)\n",
    "print(np.unique(label))\n",
    "# there are 16 classes in this shape seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is the xyz \n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_py2",
   "language": "python",
   "name": "tensorflow_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
